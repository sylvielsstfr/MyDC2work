{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Cl on galaxy overdensities on DC2-Data\n",
    "\n",
    "- author : Sylvie Dagoret-Campagne\n",
    "- affliliation : IJCLab/in2p3/CNRS\n",
    "- creation date : July 28th 2020\n",
    "\n",
    "\n",
    "- Run on Spark cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyarrow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf \n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "from pyspark.sql.types import IntegerType,FloatType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is how we create a function ('Ang2Pix') that can be called by dataframes\n",
    "# it takes as input the \"ra\" and \"dec\" values (which are not very different from theta/phi)\n",
    "# and returns the pixel number (but as pandas series for efficiency)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy as hp\n",
    "\n",
    "nside=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "npix = hp.nside2npix(nside)\n",
    "lmax = 3 * nside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ang2Pix_func(ra: pd.Series, dec: pd.Series) -> pd.Series:\n",
    "    return pd.Series(hp.ang2pix(nside,np.radians(90-dec),np.radians(ra)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_ang2pix = pandas_udf(Ang2Pix_func, returnType=IntegerType())\n",
    "#gal = gal.withColumn(\"ihealpix\",pd_ang2pix(gal[\"RA\"],gal[\"DEC\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Found 18 items\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2020-06-12 10:23 /lsst/DC2/DR6axCdc2.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2020-04-23 17:15 /lsst/DC2/cosmoDC2\n",
      "-rw-r--r--   3 stephane.plaszczynski lsst 83486737444 2020-05-28 19:33 /lsst/DC2/dc2_object_run2.2i_dr3.parquet\n",
      "-rw-r--r--   3 stephane.plaszczynski lsst 44384598496 2020-06-05 13:15 /lsst/DC2/dc2_object_run2.2i_dr6.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2020-06-26 10:36 /lsst/DC2/dc2_object_run2.2i_dr6b\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2020-07-28 18:46 /lsst/DC2/dc2_object_run2.2i_dr6c\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2019-10-21 15:54 /lsst/DC2/df1.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2019-11-08 16:58 /lsst/DC2/df2.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2019-12-06 18:48 /lsst/DC2/df3.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2019-12-10 11:35 /lsst/DC2/df4.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2019-09-24 11:18 /lsst/DC2/object_catalog\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2019-12-13 15:52 /lsst/DC2/refcatXobj.parquet\n",
      "-rw-r--r--   3 stephane.plaszczynski lsst  3628146240 2019-12-09 19:05 /lsst/DC2/refcat_v3_dc2_r2p1i.fits\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2019-12-13 13:48 /lsst/DC2/refcat_v3_dc2_r2p1i.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2020-06-08 15:29 /lsst/DC2/run21xstars.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2020-06-09 13:03 /lsst/DC2/run22xCdc2_r0.5.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2020-06-08 17:21 /lsst/DC2/run22xstars.parquet\n",
      "drwxr-xr-x   - stephane.plaszczynski lsst           0 2019-12-13 13:39 /lsst/DC2/starsXcosmoDC2.parquet\n"
     ]
    }
   ],
   "source": [
    "! hadoop dfs -ls /lsst/DC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile=\"/lsst/DC2/dc2_object_run2.2i_dr3.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the data as DataFrame\n",
    "df = spark.read.format(\"parquet\").load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DC2 Object catalog Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what we have in the file\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gal=df.filter('extendedness == true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all available tracts\n",
    "df_gal.select('tract').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat=df_gal.groupBy(\"tract\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_stat=df_stat.toPandas()\n",
    "dp_stat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=np.arange(len(dp_stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=0.8\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(indexes,dp_stat[\"count\"].values,width,color='red')\n",
    "ax.set_xticks(indexes)\n",
    "ax.set_title(\"Number of count of extended obj per tract\")\n",
    "xtickNames = ax.set_xticklabels(dp_stat[\"tract\"].values)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DM stack includes functionality to get the tract and patch number corresponding to a certain position `(RA,DEC)`. However, it is out of the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark provides `filter` mechanisms, which you can use to speed up data access if you only need a certain chunks of the dataset.\n",
    "For the object catalog, the chunks are broken into `tract` and `patch`, and hence those are the `filters` you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "for the_tract in dp_stat[\"tract\"].values:\n",
    "    selection_name = \"tract == {}\".format(the_tract)\n",
    "\n",
    "    # Retrieve the ra,dec coordinates of all sources within tract number 4430\n",
    "    data = df_gal.select('ra', 'dec').where(selection_name).collect()\n",
    "    #data = df.select('ra', 'dec').collect()\n",
    "    \n",
    "    print(\"======= tract {} ================\".format(the_tract))\n",
    "    #data.describe().show()\n",
    "\n",
    "    if dp_stat[\"count\"].values[idx]>1e4:\n",
    "    \n",
    "        # `collect` returns list of list[ra, dec], so for \n",
    "        # plotting purpose we tranpose the output:\n",
    "        ra, dec = np.transpose(data)\n",
    "\n",
    "        # Plot a 2d histogram of sources\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.hist2d(ra, dec, 100)\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.colorbar(label='Number of objects')\n",
    "        plt.xlabel('RA [deg]')\n",
    "        plt.ylabel('dec [deg]');\n",
    "        plt.title(\"tract = {}\".format(the_tract))\n",
    "        plt.show()\n",
    "        \n",
    "    idx+=1\n",
    "    if idx>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_ang2pix = pandas_udf(Ang2Pix_func, returnType=IntegerType())\n",
    "df_gal_healpix = df_gal.withColumn(\"ihealpix\",pd_ang2pix(df_gal[\"ra\"],df_gal[\"dec\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gal_healpix.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gal_healpix_small=df_gal_healpix.select('ra','dec','ihealpix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gal_healpix_small.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=df_gal_healpix_small.groupBy('ihealpix').count()\n",
    "m.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.orderBy('ihealpix', ascending=True).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get to python world (to Pandas) : \n",
    "# note that here is the action (lazy evaluation so far)\n",
    "p=m.toPandas()\n",
    "p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following doesn't has anything to do with spark (only Healpix)\n",
    "hpMap = np.zeros(hp.nside2npix(nside))\n",
    "#fill the map from the pandas object\n",
    "hpMap[p['ihealpix'].values]=p['count'].values\n",
    "#plot using standard healpy function\n",
    "hp.mollview(hpMap,cmap=\"jet\")\n",
    "hp.graticule(color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(hpMap,bins=int(100))\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = hp.anafast(hpMap)\n",
    "ell = np.arange(len(cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ell,cl,\"ro\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"$\\ell$\")\n",
    "plt.ylabel(\"$C_\\ell$\")\n",
    "plt.title(\"{} : $C_\\ell$ by healpix/anafast\".format(\"DC2Run2.2i\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpMap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
